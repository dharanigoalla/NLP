{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5EIPKj3EABq",
        "outputId": "f7584f86-d2da-40b8-d83b-f1f71740ad09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.4166\n",
            "Epoch 2/10, Loss: 0.2769\n",
            "Epoch 3/10, Loss: 0.2200\n",
            "Epoch 4/10, Loss: 0.1768\n",
            "Epoch 5/10, Loss: 0.1507\n",
            "Epoch 6/10, Loss: 0.1282\n",
            "Epoch 7/10, Loss: 0.1098\n",
            "Epoch 8/10, Loss: 0.0995\n",
            "Epoch 9/10, Loss: 0.0933\n",
            "Epoch 10/10, Loss: 0.0904\n",
            "Test Accuracy: 0.8433\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import nltk\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Download the names dataset from NLTK\n",
        "nltk.download('names')\n",
        "\n",
        "# Get the male and female names from the NLTK dataset\n",
        "male_names = [name.lower() for name in nltk.corpus.names.words('male.txt')]\n",
        "female_names = [name.lower() for name in nltk.corpus.names.words('female.txt')]\n",
        "\n",
        "# Create the list of all names and corresponding labels\n",
        "all_names = male_names + female_names\n",
        "all_labels = [0] * len(male_names) + [1] * len(female_names)\n",
        "\n",
        "# Encode labels into numerical format\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(all_labels)\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "train_names, test_names, train_labels, test_labels = train_test_split(\n",
        "    all_names, encoded_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the tokenizer from BERT\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a custom PyTorch dataset for the names\n",
        "class NameClassificationDataset(Dataset):\n",
        "    def __init__(self, names, labels, tokenizer):\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.names[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        tokenized_data = self.tokenizer(\n",
        "            name,\n",
        "            padding='max_length',\n",
        "            max_length=10,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': tokenized_data['input_ids'].squeeze(0),\n",
        "            'attention_mask': tokenized_data['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "train_data_loader = DataLoader(NameClassificationDataset(train_names, train_labels, bert_tokenizer), batch_size=32, shuffle=True)\n",
        "test_data_loader = DataLoader(NameClassificationDataset(test_names, test_labels, bert_tokenizer), batch_size=32, shuffle=False)\n",
        "\n",
        "bert_classifier = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "bert_classifier.to(device)\n",
        "\n",
        "\n",
        "optimizer = AdamW(bert_classifier.parameters(), lr=1e-5)\n",
        "\n",
        "# Train the model\n",
        "bert_classifier.train()\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in train_data_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        label = batch['label'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = bert_classifier(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=label\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_data_loader):.4f}\")\n",
        "\n",
        "\n",
        "bert_classifier.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = bert_classifier(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "\n",
        "        logits = outputs.logits\n",
        "        batch_predictions = torch.argmax(logits, axis=-1).tolist()\n",
        "\n",
        "        predictions.extend(batch_predictions)\n",
        "        true_labels.extend(labels.tolist())\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    }
  ]
}